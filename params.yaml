TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 500
  per_device_train_batch_size: 1
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: steps
  eval_steps: 500
  save_steps: 1e6
  gradient_accumulation_steps: 16

EvaluationArguments:
  batch_size: 16    
  column_text: "article"
  column_summary: "highlights"
  tokenizer_max_length: 1024 
  truncation: True
  padding: "max_length"
  return_tensors: "pt"
  length_penalty: 0.8
  num_beams: 8
  summery_max_length: 128
  skip_special_tokens: True
  clean_up_tokenization_spaces: True
  metric_batch_size: 2
  metric_column_text: 'dialogue'
  metric_column_summary: 'summary'